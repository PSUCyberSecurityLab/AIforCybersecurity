from cnn import CNN_MAL
from rnn import RNN_MAL
from gnn import GNN_MAL
import config
import pickle
import glob
import numpy as np
import tensorflow as tf
from text_iterator import text_iterator
from graph_iterator import *
from tf_utils import check_distribution
from sklearn.model_selection import KFold
#print(tf.executing_eagerly())

fold = 0
def get_fold():
    with open(config.CFG_split_data+'-kfold-'+str(fold)+'.txt', 'rb') as jf:
        train_set, val_set, test_set, train_lbs, val_lbs, test_lbs = pickle.load(jf)
    b = glob.glob('data/CFG_hex/Benign/*')
    v = glob.glob('data/CFG_hex/Virus/*')
    alls = ['data/binary/'+s.split('data/CFG_hex/')[-1] for s in b + v]
    check_distribution(train_set, alls)
    check_distribution(val_set, alls)
    check_distribution(test_set, alls)
    train_set = [alls[f] for f in train_set]
    val_set = [alls[f] for f in val_set]
    test_set = [alls[f] for f in test_set]#[:50]
    print("====== ")
    check_distribution(train_set)
    print(np.sum(np.array(train_lbs)==0), np.sum(np.array(train_lbs)==1))
    print("====== ")
    check_distribution(val_set)
    print(np.sum(np.array(val_lbs)==0), np.sum(np.array(val_lbs)==1))
    print("====== ")
    check_distribution(test_set)
    print(np.sum(np.array(test_lbs)==0), np.sum(np.array(test_lbs)==1))
    print("====== ")
    return train_set, val_set, test_set, train_lbs, val_lbs, test_lbs

def prepare_data_image(tensor = True):
    train_set, val_set, test_set, train_lbs, val_lbs, test_lbs = get_fold()
    train_set = ['data/L/'+f.split('data/binary/')[-1]+'_L.png' for f in train_set]
    val_set = ['data/L/'+f.split('data/binary/')[-1]+'_L.png' for f in val_set]
    test_set = ['data/L/'+f.split('data/binary/')[-1]+'_L.png' for f in test_set]
    def _parsefunc(filename, lbs):
       img_st = tf.io.read_file(filename)
       img_dec = tf.io.decode_png(img_st, channels=config.channel)
       img = tf.cast(img_dec,tf.float32)/255.
       img = tf.image.resize(img, [config.img_height, config.img_height],method='bilinear')
       return img, lbs
    if tensor:
        return convert_dataset(train_set, train_lbs, _parsefunc), convert_dataset(val_set, val_lbs, _parsefunc), convert_dataset(test_set, test_lbs, _parsefunc, 1)
    else:
        return train_set, val_set, ([_parsefunc(fn, -1)[0] for fn in test_set], [lbs for lbs in test_lbs])

def convert_dataset(train_set, train_lbs, _parsefunc, batch_size=config.batch_size):
    AUTOTUNE = tf.data.experimental.AUTOTUNE
    ds = tf.data.Dataset.from_tensor_slices((train_set, train_lbs))
    train_ds = ds.map(_parsefunc)
    train_ds = train_ds.batch(batch_size)
    train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)
    return train_ds

def prepare_data_txt():
    with open(config.CFG_split_data, 'rb') as jf:
        train_set, test_set, _, _ = pickle.load(jf)
    b = glob.glob('data/Seq-ori/Benign/*')
    v = glob.glob('data/Seq-ori/Virus/*')
    alls = b + v
    check_distribution(train_set, alls)
    check_distribution(test_set, alls)
    train_set = [alls[f] for f in train_set]
    test_set = [alls[f] for f in test_set]#[:50]
    check_distribution(train_set)
    check_distribution(test_set)
    print(len(train_set), len(test_set))
    train_ds = text_iterator(train_set, 1, config.batch_size)
    test_ds = text_iterator(test_set, 1, config.batch_size, shuffle=False)
    return train_ds, test_ds

def prepare_data_graph():
    with open(config.graph_data, 'rb') as jf:
        g_list = pickle.load(jf)
    with open(config.CFG_split_data, 'rb') as jf:
        train_set, test_set, _, _ = pickle.load(jf)
    print(len(train_set), len(test_set))
    return [g_list[i] for i in train_set], [g_list[i] for i in test_set]   


def prepare_opcode_data_graph():
    with open(config.graph_opcode_data, 'rb') as jf:
        g_list = pickle.load(jf)
    with open(config.CFG_split_data, 'rb') as jf:
        train_set, test_set, _, _ = pickle.load(jf)
    print(len(train_set), len(test_set))
    return [g_list[i] for i in train_set], [g_list[i] for i in test_set]   

def cnn_model(load=""):
    model = CNN_MAL(config.num_classes)
    model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
    if load!="":
        _, ckpt_manager = load_ckpt(model, config.model_save_path+load+'/')
    return model

def rnn_model(name,load=""):
    model = RNN_MAL(config.num_classes, name)
    model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['acc'])  
    if load!="":
        _, ckpt_manager = load_ckpt(model, config.model_save_path+load+'/')
    return model

def mlp_model(load=""):
    model = MLP_MAL(config.num_classes)
    model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['acc'])  
    if load!="":
        _, ckpt_manager = load_ckpt(model, config.model_save_path+load+'/')
    return model

def gnn_model(load=""):
    model = GNN_MAL(config.num_classes)
    if load!="":
        _, ckpt_manager = load_ckpt(model, config.model_save_path+load+'/')
    return model

def gcn_model(load=""):
    model = GCN_MAL(config.num_classes)
    if load!="":
        _, ckpt_manager = load_ckpt(model, config.model_save_path+load+'/')
    return model

def k_fold():
    with open(config.CFG_split_data, 'rb') as jf:
        train_set, test_set, lbs_train, lbs_test = pickle.load(jf)
    paths = np.array(train_set + test_set)
    lbs = np.array(lbs_train + lbs_test)
    kf = KFold(n_splits=5)
    i=0
    from sklearn.model_selection import train_test_split
    for train_index, test_index in kf.split(paths):
        train_graph_paths, test_set = paths[train_index], paths[test_index]
        lbs_train, lbs_test = lbs[train_index], lbs[test_index]
        val_graph_paths, test_graph_paths, lbs_val, lbs_test = train_test_split(test_set, lbs_test, test_size=0.7)
        k = (train_graph_paths, val_graph_paths, test_graph_paths, lbs_train, lbs_val, lbs_test)
        print(len(val_graph_paths),len(test_graph_paths), len(train_graph_paths))
        with open(config.CFG_split_data+'-kfold-'+str(i)+'.txt', 'wb') as jf:
            pickle.dump(k, jf)
        i+=1

fn = tf.keras.metrics.FalseNegatives()
fp = tf.keras.metrics.FalsePositives()
tn = tf.keras.metrics.TrueNegatives()
tp = tf.keras.metrics.TruePositives()
acc = tf.keras.metrics.Accuracy() 
recall = tf.keras.metrics.Recall()
def metrics(y_true, y_pred):
    print(fn(y_true, y_pred),fp(y_true, y_pred),tn(y_true, y_pred),tp(y_true, y_pred) ,recall(y_true, y_pred), acc(y_true, y_pred))
    fp.reset_states()
    fn.reset_states()
    tp.reset_states()
    tn.reset_states()
    acc.reset_states()
    recall.reset_states()

def train_gnn(name):
    with open(config.graph_opcode_data, 'rb') as jf:
        g_list = pickle.load(jf)
    for k in range(5):
        with open(config.CFG_split_data+'-kfold-'+str(k)+'.txt', 'rb') as jf:
            train_graph_paths, val_graph_paths, test_graph_paths, _, _, _=pickle.load(jf)
        k+=1
        train_ds, test_ds, val_ds = [g_list[i] for i in train_graph_paths], [g_list[i] for i in test_graph_paths], [g_list[i] for i in val_graph_paths]
        config.feature_dim = config.opcode_len
        if name == 'gcn-opcode':
            model = gcn_model()
        else:
            model = gnn_model()
        _, ckpt_manager = load_ckpt(model, config.model_save_path+name+'/kfold-'+str(k)+'/')
        _max_acc = 0.90
        for _ in range(config.epochs):
            gnn_train(model, train_ds)
            print('train')
            y_true, y_pred = gnn_test(model, train_ds)
            metrics(y_true, y_pred)
            print('test')
            y_true, y_pred = gnn_test(model, test_ds)
            metrics(y_true, y_pred)
            print('val')
            y_true, y_pred = gnn_test(model, val_ds)
            metrics(y_true, y_pred)
            ckpt_manager.save()

def train(name):
    if name == 'cnn':
        train_ds, val_ds, _ = prepare_data_image()
        model = cnn_model()
    elif name == 'lstm' or name == 'gru' or name == 'rnn':
        train_ds, val_ds, _ = prepare_data_txt()
        model = rnn_model(name)
    elif name == 'mlp':
        train_ds, val_ds, _ = prepare_data_image()
        model = mlp_model()
    elif name == 'gnn':
        train_ds, val_ds, _ = prepare_data_graph()
        model = gnn_model()
    elif name == 'gnn-opcode':
        config.feature_dim = config.opcode_len
        train_ds, val_ds, _ = prepare_opcode_data_graph()
        model = gnn_model()
    
    _, ckpt_manager = load_ckpt(model, config.model_save_path+name+'/')
    _max_acc = 0.90
    if name == 'gnn' or name == 'gnn-opcode':
        for _ in range(config.epochs):
            gnn_train(model, train_ds)
            y_true, y_pred = gnn_test(model, val_ds)
            metrics(y_true, y_pred)
            ckpt_manager.save()
    else:
        for i in range(config.epochs):
            print('===========\n',i)
            history = model.fit(
                train_ds,
                validation_data=val_ds,
                batch_size=config.batch_size,
                #steps_per_epoch=config.train_size/config.batch_size,
                epochs=5#config.epochs
            ) 
            if history.history['val_accuracy'][0] > _max_acc:
                _max_acc = history.history['val_accuracy'][0]     
                ckpt_manager.save()       

def evaluation(name):
    if name == 'cnn':
        train_ds, _, val_ds = prepare_data_image()
        model = cnn_model(name)
    elif name == 'lstm' or name == 'gru' or name == 'rnn':
        train_ds, _, val_ds = prepare_data_txt()
        model = rnn_model(name,name)
    elif name == 'mlp':
        train_ds, _, val_ds = prepare_data_image()
        model = mlp_model(name)
    elif name == 'gnn':
        train_ds, _, val_ds = prepare_data_graph()
        model = gnn_model(name)
    elif name == 'gnn-opcode':
        train_ds, _, val_ds = prepare_opcode_data_graph()
        model = gnn_model(name)

    
    if name == 'gnn':
        y_true, y_pred = gnn_test(model, val_ds)
        metrics(y_true, y_pred)
    elif name == 'lstm' or name == 'gru' or name == 'rnn':
        y_pred = model.predict(val_ds)
        y_pred = tf.reshape(tf.argmax(y_pred, axis=1), shape=(-1))
        y_true = val_ds.get_labels()
        y_true=tf.concat(y_true, 0)
        metrics(y_true, y_pred)
    else:
        y_pred = model.predict(val_ds)
        model.save(config.model_save_path+name+"/single.model")
        y_pred = tf.reshape(tf.argmax(y_pred, axis=1), shape=(-1))
        y_true = []
        for element in val_ds.as_numpy_iterator(): 
            y_true.append(element[1]) 
        y_true=tf.concat(y_true, 0)
        metrics(y_true, y_pred)

def load_ckpt(model, path, newest=True):
    ckpt = tf.train.Checkpoint(transformer=model)
    ckpt_manager = tf.train.CheckpointManager(ckpt, path, max_to_keep=5)
    if ckpt_manager.latest_checkpoint:
        if newest == True:
            ckpt.restore(ckpt_manager.latest_checkpoint)
            print ('Latest checkpoint restored!!', path, newest, ckpt_manager.latest_checkpoint)
        else:
            ckpt.restore(path+newest)
            print ('checkpoint restored!!', path, newest)
    return ckpt, ckpt_manager

if __name__ == '__main__':
    train('cnn')
    #train_gnn('gnn-opcode')
    #evaluation('cnn')
